{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.4.1 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: tensorflow-gpu==2.4.1 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\aastha\\anaconda3\\lib\\site-packages (4.5.4.58)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\aastha\\anaconda3\\lib\\site-packages (0.8.8.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\aastha\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aastha\\appdata\\roaming\\python\\python38\\site-packages (3.4.2)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.32.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.15.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.12)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.3.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.19.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.7.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.4.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.35.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.19.5)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from mediapipe) (20.3.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from mediapipe) (4.5.4.58)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\aastha\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aastha\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aastha\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aastha\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aastha\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (50.3.1.post20201107)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.3.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2020.6.20)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.7.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aastha\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "# Import and install dependencies.\n",
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will setup Media Pipe Holistic\n",
    "\n",
    "#This is mediapipe holistic model which makes the detections.\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "#This is mediapipe holistic drawing utilities which will draw the detection.\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #This is color coversion from BGR to RGB.\n",
    "    image.flags.writeable = False  #image is no longer writeable.\n",
    "    results = model.process(image) #Make prediction.\n",
    "    image.flags.writeable = True  #Image is now writeable.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #This is color conversion from RGB to BGR.\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first we will access webcam using cv2 </br>\n",
    "#### This is the method for accessing the web cam we will use this method to collect dataset for our model to train</br>\n",
    "cap = cv2.VideoCapture(0)</br>\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        #capturig frames from the webcam\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #between read and rendering we want to make our prediction.\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        #Draw ladmarks in real time\n",
    "        enhanced_landmarks(image, results)\n",
    "\n",
    "        #Showing frames on the screen.\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        #Breaking gracefully from the loop.\n",
    "        #This line specifies that when we want to break out of loop we need to press 'q' on the keyboard.\n",
    "        if cv2.waitKey(10) & 0XFFF == ord('q'):\n",
    "            break;   \n",
    "    cap.release()\n",
    "    #This line of code closes alll the windows.\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "#### Then on above that layer we will apply mediapipe <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    #Draw Face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    #Draw pose Connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    #Draw left Hand Connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    #Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245, 117, 76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method is for extracting keypoints from our media pipe detection for further recognition.\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actions that we try to detect\n",
    "#First we will train our model for digits\n",
    "actions = np.array(['0','1','2','3','4','5','6','7','8','9'])\n",
    "#Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "#videos are going to be 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actions that we try to detect\n",
    "#First we will train our model for digits\n",
    "#actions = np.array(['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',])\n",
    "#actions = np.array(['Hello', 'India', 'Sign', 'Language','Bye','Again','I','You','Man','Woman','He','She','Deaf','Hearing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: MP_Data : The system cannot find the file specified\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.rmdir(DATA_PATH)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (DATA_PATH, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This traverse through every actions or labels and inside that action there will be thirty videos, each of thirty frames and \n",
    "#Creates the folder or directory for the same\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, actions , str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Aastha\\\\Documents\\\\GitHub\\\\Real-Time-SignLanguageDetection'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data\\\\0\\\\0\\\\0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4a8e5e9c2df4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mkeypoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_keypoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mnpy_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpy_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[0mfile_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data\\\\0\\\\0\\\\0.npy'"
     ]
    }
   ],
   "source": [
    "#This method is for collecting keyframes or dataset for the acitons or lables specified in the actions variable above\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #Loop through actions\n",
    "    for action in actions:\n",
    "        #Loop through each videos\n",
    "        for sequence in range(no_sequences):\n",
    "            #Loop through video length\n",
    "            for frame_num in range(sequence_length):\n",
    "                #capturig frames from the webcam\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                #between read and rendering we want to make our prediction.\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "\n",
    "                #Draw ladmarks in real time\n",
    "                draw_landmarks(image, results)\n",
    "\n",
    "                #Apply wait Logic.\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} video Number {}'.format(action, sequence), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                #Now we will extract keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "\n",
    "                #Showing frames on the screen.\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                #Breaking gracefully from the loop.\n",
    "                #This line specifies that when we want to break out of loop we need to press 'q' on the keyboard.\n",
    "                if cv2.waitKey(10) & 0XFFF == ord('q'):\n",
    "                    break;  \n",
    "                    \n",
    "    cap.release()\n",
    "    #This line of code closes alll the windows.\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "#This line of code closes alll the windows.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map\n",
    "\n",
    "\n",
    "#Now we will bring our data together and start structuring\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "        \n",
    "        \n",
    "np.array(sequences).shape\n",
    "\n",
    "np.array(labels).shape\n",
    "\n",
    "x = np.array(sequences)\n",
    "x.shape\n",
    "\n",
    "y = to_categorical(labels).astype(int)\n",
    "y\n",
    "\n",
    "#Now we will do training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05)\n",
    "\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "x_test.shape\n",
    "\n",
    "\n",
    "y_train.shape\n",
    "\n",
    "y_test.shape\n",
    "\n",
    "\n",
    "#Build the LSTM model\n",
    "\n",
    "# For this purpose we are going to use tensorflow and keras\n",
    "#first of all we have to import some dependencies that is \n",
    "#Sequencial model\n",
    "#LSTM model\n",
    "#Dense layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard #This gonna allow some logins in tesorflow\n",
    "\n",
    "\n",
    "#Now we will create a log directory and setup our tensorflow callbacks\n",
    "#log directory allows us to monitor our neural network training\n",
    "log_dir = os.path.join('logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "\n",
    "#Now we will build our neural network architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "\n",
    "#Now we will compile our model and fit it\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=850, callbacks=[tb_callback])\n",
    "\n",
    "\n",
    "\n",
    "#Now Testing our model\n",
    "res = model.predict(x_test)\n",
    "actions[np.argmax(res[30])]\n",
    "actions[np.argmax(y_test[30])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
